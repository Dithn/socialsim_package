{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json \n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from socialsim_scoring.cp4 import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the narratives you want to include in the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives = ['arrests',\n",
    "            'arrests/opposition',\n",
    "            'guaido/legitimate',\n",
    "            'international/aid',\n",
    "            'international/aid_rejected',\n",
    "            'international/respect_sovereignty',\n",
    "            'maduro/cuba_support',\n",
    "            'maduro/dictator',\n",
    "            'maduro/legitimate',\n",
    "            'maduro/narco',\n",
    "            'military',\n",
    "            'military/desertions',\n",
    "            'other/anti_socialism',\n",
    "            'other/censorship_outage',\n",
    "            'other/chavez',\n",
    "            'other/chavez/anti',\n",
    "            'protests',\n",
    "            'violence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_list = ['number_of_shares','number_of_shares_over_time','activated_users','activated_users_over_time',\n",
    "            'degree_distribution','page_rank']\n",
    "meas_list_scalar = ['number_of_shares','activated_users']\n",
    "meas_list_temporal = ['number_of_shares_over_time','activated_users_over_time']\n",
    "save_plots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Specify file path and pickled measurement and metric results files to load. This notebook for analyzing performance of multiple models across a single split.  Please point the notebook to the files for a single split. To generate these pickle files, run the EvaluationRunner (see the example in /examples/eval_runner.py) and pickle the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "fns = glob.glob(path + f'*measurement*.pickle')\n",
    "fns = sorted(fns)\n",
    "fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_metrics(fns, narratives,platforms=['twitter','youtube'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df['split'].nunique() > 1:\n",
    "    print('More than one split found. This notebook is for analyzing one split at a time. Please refine file list above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric-Level Plots\n",
    "Plot the metric performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['narrative','model','measurement',\n",
    "            'metric','platform'])['value'].agg([np.mean,np.std]).reset_index()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric distributions by model\n",
    "\n",
    "The plots below show the variability in metrics results across narratives and models.  Each point is an indvidual narrative and the error bars show the variability across multiple submissions of the same method. You can specify which platforms and measurements you would like to plot.  Additionally, some measurements tend to show a large range of metric results, so there is a log option to plot these metrics on a log scale to better see this range (if log=True the metrics than span more than a factor of 100 will be plotted on log scale while the others will remain on linear scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "strip_plot(grouped,['twitter','youtube'],meas_list,log=True,save_plots=save_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric distributions by model: narrative focus\n",
    "\n",
    "We can also highlight individual narratives in these plots.  The narrative variable in the block below can be used to select a specific narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative = 'arrests'\n",
    "platform = 'twitter'\n",
    "meas = 'number_of_shares'\n",
    "    \n",
    "strip_plot(grouped,[platform],[meas],narrative=narrative,log=False,save_plots=save_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise model comparisons\n",
    "\n",
    "The plots below show a scatter plot comparison of each pair of models, with the values plotted being the metric result.  Each point is a single narrative and the error bars represent variation among multiple submissions from the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if grouped['model'].nunique() > 1:\n",
    "    pairwise_scatterplots(grouped,['twitter','youtube'],meas_list,log=True,save_plots=save_plots)\n",
    "else:\n",
    "    print('Must have more than one model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCDF Plots\n",
    "\n",
    "The plots below show a comparison of the distribution of metrics across narratives for the different models. The line shows the percentage of narratives that have an error value as good or better than the value on the x-axis for a given model. The error band indicates variability due to multiple submissions from the same model. As with the above plots, you can set the log option to True to apply a log scale to any metrics which span more than a factor of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ccdf_plots(df,['twitter','youtube'],meas_list,log=True,save_plots=save_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement-Level Plots\n",
    "Plot the measurements of the model and the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df, sim_df = load_measurements(fns,narratives,meas_list_scalar,meas_list_temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_time_df = sim_df.groupby(['platform','informationID','meas',\n",
    "                              'nodeTime','model'])['value'].agg([np.mean,np.std]).reset_index()\n",
    "gt_time_df = gt_df.groupby(['platform','informationID','meas',\n",
    "                              'nodeTime'])['value'].mean().reset_index()\n",
    "\n",
    "sim_time_df = sim_time_df.merge(gt_time_df,on=['platform','informationID','meas','nodeTime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Measurements\n",
    "\n",
    "The plots below compare the ground truth time series measurements with the simulation measurements for temporal measurements. The specific narrative to plot can be specified using the narrative variable in the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meas = 'number_of_shares_over_time'\n",
    "platform = 'twitter'\n",
    "narrative = 'arrests'\n",
    "\n",
    "time_series_plot(sim_time_df,platform,meas,narrative,save_plots=save_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meas = 'number_of_shares_over_time'\n",
    "platform = 'youtube'\n",
    "\n",
    "time_series_plot(sim_time_df,platform,meas,narrative,save_plots=save_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meas = 'activated_users_over_time'\n",
    "platform = 'twitter'\n",
    "\n",
    "time_series_plot(sim_time_df,platform,meas,narrative,save_plots=save_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meas = 'activated_users_over_time'\n",
    "platform = 'youtube'\n",
    "\n",
    "time_series_plot(sim_time_df,platform,meas,narrative,save_plots=save_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scalar_df = sim_df[sim_df['nodeTime'].isnull()].drop('nodeTime',axis=1)\n",
    "gt_scalar_df = gt_df[gt_df['nodeTime'].isnull()].drop('nodeTime',axis=1)\n",
    "\n",
    "sim_scalar_df = sim_scalar_df.groupby(['platform','informationID',\n",
    "                                       'meas','model'])['value'].agg([np.mean,np.std]).reset_index()\n",
    "gt_scalar_df = gt_scalar_df.groupby(['platform','informationID','meas'])['value'].mean().reset_index()\n",
    "\n",
    "sim_scalar_df = sim_scalar_df.merge(gt_scalar_df,on=['platform','informationID','meas'])\n",
    "sim_scalar_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar measurements plot\n",
    "The below plots show the measured value of the scalar measurements in the simulation versus the ground truth value.  Each point is an individual narrative and the error bars indicate variation over multiple submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scatter_plot(sim_scalar_df,['twitter','youtube'],meas_list_scalar,log=False,save_plots=save_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
