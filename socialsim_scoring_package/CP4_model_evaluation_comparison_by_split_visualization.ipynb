{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json \n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from socialsim_scoring.cp4 import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives = ['arrests',\n",
    "            'arrests/opposition',\n",
    "            'guaido/legitimate',\n",
    "            'international/aid',\n",
    "            'international/aid_rejected',\n",
    "            'international/respect_sovereignty',\n",
    "            'maduro/cuba_support',\n",
    "            'maduro/dictator',\n",
    "            'maduro/legitimate',\n",
    "            'maduro/narco',\n",
    "            'military',\n",
    "            'military/desertions',\n",
    "            'other/anti_socialism',\n",
    "            'other/censorship_outage',\n",
    "            'other/chavez',\n",
    "            'other/chavez/anti',\n",
    "            'protests',\n",
    "            'violence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_names = ['february1-february14','february8-february21','february15-february28','february22-february28',\n",
    "              'march1-march14','march8-march21','march15-march28','march22-april4']\n",
    "\n",
    "\n",
    "meas_list = ['number_of_shares','number_of_shares_over_time','activated_users','activated_users_over_time',\n",
    "            'degree_distribution','page_rank']\n",
    "metric_list = ['APE','RMSE','APE','RMSE','RH Distance','EM Distance']\n",
    "meas_list_scalar = ['number_of_shares','activated_users']\n",
    "meas_list_temporal = ['number_of_shares_over_time','activated_users_over_time']\n",
    "save_plots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Specify file path and pickled measurement and metric results files to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_identifier = ''\n",
    "path = './'\n",
    "fns = glob.glob(path + f'*{model_identifier}*measurement*.pickle')\n",
    "fns = sorted(fns)\n",
    "\n",
    "fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_metrics(fns, narratives,platforms=['twitter','youtube'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df['model'].nunique() > 1:\n",
    "    print('More than one model found. This notebook is for analyzing one model at a time. Please refine file list above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify path to baseline metrics and measurement pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_path = './'\n",
    "bl_fns = glob.glob(bl_path + '*.pkl')\n",
    "bl_fns = sorted(bl_fns)\n",
    "bl_df = load_metrics(bl_fns, narratives,platforms=['twitter','youtube'])\n",
    "bl_df = bl_df[bl_df['split'].isin(df['split'])]\n",
    "\n",
    "bl_grouped = bl_df.groupby(['narrative','split','measurement',\n",
    "            'metric','platform'])['value'].agg([np.mean,np.std]).reset_index()\n",
    "\n",
    "bl_grouped = bl_grouped.rename(columns={'mean':'bl_mean',\n",
    "                                        'std':'bl_std'})\n",
    "\n",
    "\n",
    "bl_grouped['split'] = pd.Categorical(bl_grouped['split'], \n",
    "                      categories=split_names,\n",
    "                      ordered=True)\n",
    "\n",
    "bl_grouped = bl_grouped.sort_values('split')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric-Level Plots\n",
    "Plot the metric performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['model','narrative','split','measurement',\n",
    "            'metric','platform'])['value'].agg([np.mean,np.std]).reset_index()\n",
    "\n",
    "grouped = grouped.rename(columns={'mean':'sim_mean',\n",
    "                                 'std':'sim_std'})\n",
    "\n",
    "\n",
    "grouped['split'] = pd.Categorical(grouped['split'], \n",
    "                      categories=split_names,\n",
    "                      ordered=True)\n",
    "\n",
    "grouped = grouped.sort_values('split')\n",
    "\n",
    "\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge simulation and baseline data\n",
    "grouped = grouped.merge(bl_grouped,on=['narrative','split','measurement','metric','platform'])\n",
    "df_sim = pd.concat([df,bl_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric distributions by time split\n",
    "\n",
    "The plots below show the variability in metrics results across narratives and time splits for both the simulation (orange) and the baseline (blue).  Each point is an indvidual narrative and the error bars show the variability across multiple submissions of hte same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "strip_plot_by_split(grouped,['twitter','youtube'],meas_list,metric_list,split_names=split_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric distributions by time split: narrative focus\n",
    "\n",
    "We can also highlight individual narratives in these plots.  The narrative variable in the block below can be used to select a specific narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'twitter'\n",
    "narrative = 'arrests'\n",
    "    \n",
    "for meas in meas_list:\n",
    "    metric = metric_list[meas_list.index(meas)]\n",
    "    strip_plot_by_split(grouped,[platform],[meas],[metric],split_names,narrative=narrative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric scatter plots\n",
    "\n",
    "The below plots show how the simulation performance compares with the baseline by plotting a scatterplot of the simulation metric result versus the baseline metric results at the narrative level.  The error bars indicate the variation due to multiple submissions from the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_scatterplot(grouped,['twitter','youtube'],meas_list,metric_list,split_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCDF Plots\n",
    "\n",
    "The plots below show a comparison of the distribution of metrics across narratives for the baseline (orange) and simulation (blue) for each time split. The error band indicates variability due to multiple submissions from the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_ccdf(df_sim,['twitter','youtube'],meas_list,metric_list,split_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement-Level Plots\n",
    "Plot the measurements of the model and the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df, sim_df = load_measurements(fns,narratives,meas_list_scalar,meas_list_temporal)\n",
    "_, bl_df = load_measurements(bl_fns,narratives,meas_list_scalar,meas_list_temporal)\n",
    "bl_df = bl_df[bl_df['split'].isin(sim_df['split'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_time_df = sim_df.groupby(['model','platform','informationID','meas',\n",
    "                              'nodeTime','split'])['value'].agg([np.mean,np.std]).reset_index()\n",
    "bl_time_df = bl_df.groupby(['model','platform','informationID','meas',\n",
    "                              'nodeTime','split'])['value'].agg([np.mean,np.std]).reset_index()\n",
    "bl_time_df['model'] = 'Baseline'\n",
    "\n",
    "gt_time_df = gt_df.groupby(['platform','informationID','meas',\n",
    "                              'nodeTime'])['value'].mean().reset_index()\n",
    "\n",
    "sim_time_df = sim_time_df.merge(gt_time_df,on=['platform','informationID','meas','nodeTime'])\n",
    "bl_time_df = bl_time_df.merge(gt_time_df,on=['platform','informationID','meas','nodeTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_time_df = pd.concat([sim_time_df,bl_time_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Measurements\n",
    "\n",
    "The plots below compare the ground truth time series measurements with the measurements of both the simulation and baseline across multiple (overlapping) time splits.  The baseline measurements are shown with a dotted line and the simulation for the corresponding time split is shown in the same color with a solid line. The specific narrative to plot can be specified using the narrative variable in the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meas = 'number_of_shares_over_time'\n",
    "platform = 'twitter'\n",
    "\n",
    "for narrative in sim_time_df['informationID'].unique():\n",
    "    time_series_plot_by_split(sim_time_df,platform,meas,narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas = 'number_of_shares_over_time'\n",
    "platform = 'youtube'\n",
    "\n",
    "for narrative in sim_time_df['informationID'].unique():\n",
    "    time_series_plot_by_split(sim_time_df,platform,meas,narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas = 'activated_users_over_time'\n",
    "platform = 'twitter'\n",
    "\n",
    "for narrative in sim_time_df['informationID'].unique():\n",
    "    time_series_plot_by_split(sim_time_df,platform,meas,narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas = 'activated_users_over_time'\n",
    "platform = 'youtube'\n",
    "\n",
    "for narrative in sim_time_df['informationID'].unique():\n",
    "    time_series_plot_by_split(sim_time_df,platform,meas,narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scalar_df = sim_df[sim_df['nodeTime'].isnull()].drop('nodeTime',axis=1)\n",
    "bl_scalar_df = bl_df[bl_df['nodeTime'].isnull()].drop('nodeTime',axis=1)\n",
    "\n",
    "gt_scalar_df = gt_df[gt_df['nodeTime'].isnull()].drop('nodeTime',axis=1)\n",
    "\n",
    "sim_scalar_df = sim_scalar_df.groupby(['platform','informationID',\n",
    "                                       'meas','split'])['value'].agg([np.mean,np.std]).reset_index()\n",
    "sim_scalar_df = sim_scalar_df.rename(columns={'mean':'sim_mean',\n",
    "                                             'std':'sim_std'})\n",
    "bl_scalar_df = bl_scalar_df.groupby(['platform','informationID',\n",
    "                                       'meas','split'])['value'].agg([np.mean,np.std]).reset_index()\n",
    "bl_scalar_df = bl_scalar_df.rename(columns={'mean':'bl_mean',\n",
    "                                             'std':'bl_std'})\n",
    "\n",
    "gt_scalar_df = gt_scalar_df.groupby(['platform','informationID','meas'])['value'].mean().reset_index()\n",
    "\n",
    "bl_scalar_df = bl_scalar_df.merge(gt_scalar_df,on=['platform','informationID','meas'])\n",
    "sim_scalar_df = sim_scalar_df.merge(gt_scalar_df,on=['platform','informationID','meas'])\n",
    "sim_scalar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scalar_df = sim_scalar_df.merge(bl_scalar_df,on = ['platform',\n",
    "                                                     'informationID',\n",
    "                                                     'meas',\n",
    "                                                     'split',\n",
    "                                                     'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scalar_df['delta_mean'] = sim_scalar_df['bl_mean'] - sim_scalar_df['sim_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar Measurements\n",
    "\n",
    "In the plots below we show a comparison of the scalar measurements for the simulation with the baseline values versus the ground truth values.  The colored markers show the scatter plot of the simulation measurement with the ground truth measurement for each narrative.  The arrows show how the simulation measurements are changed compared with the baseline values.  For example, if the arrows uniformly point up it means that the simulation measurements are uniformly higher than the baseline measurements for all narratives.  If the arrows all point towards the one-to-one line, it means that the simulation measurements are closer to the ground truth values than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_quiver(sim_scalar_df,['twitter','youtube'],meas_list_scalar,split_names = split_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
